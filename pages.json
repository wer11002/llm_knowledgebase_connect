[
  {
    "page_number": 1,
    "text": "Received 2 April 2024, accepted 21 May 2024, date of publication 29 May 2024, date of current version 10 June 2024.\nDigital Object Identifier 10.1109/ACCESS.2024.3406670\nStateless System Performance Prediction and\nHealth Assessment in Cloud Environments:\nIntroducing cSysGuard, an Ensemble\nModeling Approach\nNUTT CHAIRATANA\n1 AND RATHACHAI CHAWUTHAI\n2\n1Department of Robotics and AI Engineering, School of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok 10520, Thailand\n2Department of Computer Engineering, School of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok 10520, Thailand\nCorresponding author: Rathachai Chawuthai (rathachai.ch@kmitl.ac.th)\nABSTRACT Stateless cloud computing presents remarkable scalability and cost-effectiveness by offering\ndynamically adjustable resources tailored to fluctuating demands, eliminating the constraints of stateful\narchitectures. However, the challenges presented by dynamic workload are substantial in the context\nof system health monitoring, frequently leading to service interruptions owing to insufficient resources.\nIt underscores the need for the development of more efficient monitoring systems. Our study introduces\ncSysGuard, a novel framework designed to enhance monitoring capabilities within cloud environments.\nThe methodology employs an ensemble regression model with a stacking strategy to forecast dynamic\nperformance metrics. The algorithm also leverages a classification model to assess the system’s health based\non forecasted metrics, effectively identifying potential failures in the future. Under the configuration utilized,\nour evaluations demonstrated increased predictive performance with cSysGuard in forecasting various\nmetrics compared to traditional models. The results showed an improvement of up to a remarkable 2.28-fold\nincrease, varying significantly based on the specific metric under consideration. In addition, the effectiveness\nof health assessment was achieved through Decision Trees with hyperparameter tuning, resulting in a macro-\naveraged F1 score of 89.79%. This research contributes to both the theoretical and practical aspects of server\nmonitoring, presenting a solution that assesses system performance metrics and health to tackle dynamic\nchallenges in cloud infrastructure.\nINDEX TERMS Cloud computing, stacking ensemble models, machine learning, non-functional testing,\npredictive maintenance, resource allocation, proactive system metrics, stateless application.\nI. INTRODUCTION\nCloud computing, renowned for its scalability [1] and\nflexibility [2], has revolutionized the management of sys-\ntem resources. Within its broad spectrum, stateless cloud\ncomputing plays a crucial role. This paradigm facilitates\nthe flexible allocation of resources without retaining session\ninformation [3], significantly aiding in instance scaling.\nIn such cases, it significantly enhances system reliability\nand adaptability to fluctuating demands while maintaining\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Liangxiu Han\n.\noptimal system utilization. As easily integrated with various\narchitectures, stateless computing has become the preferred\nframework for application hosting, directly catering to the\ndynamic demands of the digital environment.\nNonetheless, the unpredictable nature of cloud computing\n[4] often complicates effective resource allocation, carrying\nthe risk of unexpected system failures that violate service-\nlevel agreements or SLAs [5]. Even though cloud providers\noffer auto-scaling mechanisms [6] for dynamic resource\nadjustment, they often rely on predefined rules or simple\nthreshold-based techniques [7], [8]. While certain providers\nalso provide predictive methods [9] for auto-scaling, the\n78232\n 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME 12, 2024"
  },
  {
    "page_number": 2,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nsystems are typically pattern-based [10]. These strategies\noften struggle with scaling within a reasonable timeframe\nduring irregular workload patterns, such as traffic spikes,\nleading to inefficient resource management. A more robust\nframework is required to handle these challenges in such\ncases.\nFurthermore, existing monitoring frameworks primarily\nfocus on real-time monitoring data but rarely provide\ncontinuous analysis of system health based on current\ncaptured data. This limitation arises because the interplay\nbetween resource metrics and their sufficiency assessment—\ndetermining whether they are adequate or inadequate—is\nintricate. It is often characterized by nonlinear dependencies\nfrom various factors, such as application characteristics and\nsystem configuration. Therefore, relying solely on individual\nmetrics to assess resource allocation adequacy effectively can\nbe challenging.\nTo address these challenges, we require a sophisticated\ntool that accurately forecasts key metrics and assesses system\nhealth using these indicators in dynamic cloud environments.\nIn response, we have developed cSysGuard, denoted as\n‘‘Cloud System Guard,’’ a comprehensive ensemble pre-\ndictor framework designed to handle unpredictable nature\neffectively. It builds upon our previous research [11], which\nprimarily focused on real-time system-health analysis. With\nthis advancement, cSysGuard comprises two main compo-\nnents: a system performance metric forecasting module and\na health assessment feature. It predicts performance metrics\nand then promptly uses these forecasts to assess potential\nsystem failures. Unlike traditional real-time monitoring\ntools [51], [52], cSysGuard emphasizes machine learning-\nbased prediction with customizable internal architecture. This\nflexibility allows users to tailor the configuration to their\nneeds, maximizing predictive accuracy and robustness in\nrapidly changing cloud environments.\nThis manuscript elucidates our research on stateless\napplications, introducing cSysGuard, an ensemble machine\nlearning framework devised for precise system metrics\nforecasting and health status diagnosis. The narrative begins\nwith a thorough literature review (Section II), establishing\na foundational understanding and drawing parallels with\nsimilar endeavors in the field. We then transition to a compre-\nhensive overview of cSysGuard’s methodology (Section III),\ndetailing the high-level workings, component functionalities,\nand data preprocessing strategies. The discussion elaborates\non system metrics forecasting (Section IV), highlighting the\nregression analysis framework, input-output formats, and\nthe model’s evaluation and tuning processes. Subsequently,\nwe explore the system health assessment phase (Section V),\nfocusing on classification analysis, predictor structure, and\nperformance optimization techniques. The paper culmi-\nnates in the Results and Discussion sections (Section VI),\npresenting a critical evaluation of cSysGuard’s predictive\ncapabilities, followed by a conclusion (Section VII) that\nencapsulates the essential findings and suggests avenues for\nfuture research.\nII. LITERATURE REVIEW\nThis section provides a comprehensive synthesis of cloud\ncomputing with machine-learning backgrounds, insights\nfrom prior studies, and the techniques utilized in our research.\nA. ADAPTATION: CLOUD WITH MACHINE LEARNING\nAlthough cloud applications introduce innovative resource\nmanagement strategies, they also present challenges in\nmonitoring resources. Its variable nature complicates the\neffective tracking of fluctuating system metrics. Conse-\nquently, resource utilization can result in inefficiency and\ndiminished system performance. It underscores the need\nfor more flexible monitoring methods to precisely assess\nand adapt to the ever-changing requirements of cloud\nenvironments.\nTherefore, there is growing interest in leveraging machine\nlearning techniques to address these challenges [12], [13],\n[14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24],\n[25], [26], [27], [28]. Machine learning-driven predictive\nanalytics, widely used in many fields [29], can analyze\ncomplex data patterns to identity potential anomalies. This\nproactive approach enhances system reliability and resource\nallocation, ensuring scalable and responsive services. Inte-\ngrating machine learning with cloud computing represents\na significant advancement in improving system resource\nmanagement.\nB. RELATED WORK\nNotably, many methodologies frequently favor singular\npredictive models [12], [13], [14], [15], [16], [17], [18], using\nuniversal models irrespective of specific system nuances,\na practice often known as the ‘‘one-size-fits-all’’ approach.\nHowever, as emphasized by Kim et al. [22], relying on a\nsingle predictor model does not adequately address the cloud\nworkload dynamics and short-term volatility. Singh et al.\n[19] proposed an alternative solution using a Support Vector\nMachine to categorize workloads into broad categories like\n‘‘very low,’’ ‘‘low,’’ ‘‘medium,’’ and ‘‘high.’’ Although the\nmethod offers a simplified overview of the status, the reliance\non categorical variables is insufficient for delivering precise\nvalues in applications that require detailed nuances. Gao et al.\n[20] and Caron et al. [21] proposed forecasting methods\nbased on historical pattern data. Due to their heavy reliance\non historical trends, these methods are limited in rapidly\nevolving environments, particularly when encountering novel\npatterns that emerge without precedent. In contrast, cSys-\nGuard employs an ensemble machine-learning approach that\nleverages the unique strengths of various models to achieve\nrobust predictive capabilities. This strategy effectively adapts\nto the rapidly evolving cloud environments.\nSeveral studies have proposed ensemble model approaches\nthat incorporate numerous predictors. Kim et al. [22]\nachieved balanced predictions by employing multiple models\nusing a weighted averaging strategy. This method maximizes\nthe strength of each model, calibrated by their predictive\nVOLUME 12, 2024\n78233"
  },
  {
    "page_number": 3,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 1. RMSE-based performance of meta models. This figure\nillustrates the fluctuating performances of different meta-models for CPU\nrequests (unit: percent) over 20 minutes in a dynamic environment.\nperformance. Nonetheless, as Section VI-C indicates, relying\nsolely on weighted averaging yields a lower accuracy than\ncSysGuard, where high predictive accuracy is essential\nfor anticipating potential failures. Mehmood et al. [23]\nproposed a stacking ensemble model using Decision Trees\nas a meta-model to finalize the predictions from the base\nmodels. Our findings indicate that a single meta-model\nmay not always provide robust performance, especially in\nunpredictable environments. Fig. 1 illustrates the varying\nperformances of meta-models for Central Processing Unit\n(CPU) requests in a dynamic cloud environment over time.\nThe data indicates that the best-performing meta-model\nchanges periodically, presenting a challenge to select the\nbest meta-model for the specific usage. Rather than building\nupon a single meta-model or weight averaging, cSysGuard\nemploys a multi-layered stacking ensemble approach with\na weighted averaging strategy, effectively amalgamating the\ncontributions of all models in the output generation.\nOn the other hand, deep learning techniques have been\nextensively utilized in several studies [24], [25], [26],\n[27], [28]. Although deep learning techniques are potent\ninstruments for predictive analysis, they inevitably require\nsubstantial computational resources, even when applied to\nsimpler metric patterns. Meanwhile, cSysGuard offers a\nbalanced strategy for computational resource utilization\nand predictive accuracy by judiciously selecting well-suited\nmodels with specific data characteristics. Even the best\naccuracy is considered without regard to computational\ncomplexity; cSysGuard still performs better than standalone\ndeep learning models, owing to its incorporation of a diverse\narray of predictive models.\nOur prior research focused on system health assessments.\nAlthough the inputs remain consistent with our current work,\nthe approach in this study diverges by employing forecasted\nvalues instead of actual historical data. This methodology\nintegrates our regression analytics to predict these met-\nrics and feeds the results into our classification health\nassessment process. This innovative approach enhances our\ncapability to identify potential future system failures by\nleveraging insights to inform proactive measures for resource\nmanagement.\nC. TECHNIQUE\nThis subsection provides a comprehensive overview of the\nessential techniques and a foundation for understanding the\nadvancements in our study area.\n1) LOW-PASS BUTTERWORTH FILTER\nThe low-pass Butterworth filter, a key concept in signal\nprocessing [30], attenuates high-frequency noise while\nmaintaining signal integrity. Characterized by its maximally\nflat magnitude response and ripple-free passband, the filter\nensures a smooth frequency response up to the cut-\noff frequency, making it ideal for applications requiring\nundistorted outputs. Its transfer function, defined by the\nfilter order, influences the steepness of the roll-off at\nthe cut-off point [31]. Higher-order filters offer a faster\npassband-to-stopband transition but increase the complexity\nand risk of phase distortion. In the data analysis, the filter\nactively smooths out noise and data variations, bolstering the\nreliability of the analytical results.\n2) SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE\nThe synthetic minority over-sampling technique (SMOTE)\naddresses imbalanced data by generating synthetic samples\nfor the minority class, thereby improving the data distribution\nfor machine learning applications. It creates new instances\nthrough interpolation between minority samples and their\nneighbors [32], enhancing diversity and reducing overfitting\nrisks. However, its effectiveness depends on careful parame-\nter tuning, including the number of neighbors and the amount\nof synthetic data generated. The versatility of SMOTE makes\nit applicable across various domains, from fraud detection to\nmedical diagnoses, where it significantly contributes to more\nequitable and accurate predictive modeling.\n3) ENSEMBLE MODEL: STACKING\nThe stacking approach in ensemble models features a multi-\nlayered structure, each recognized as a Level-N layer, for\nenhanced prediction. Initially, the base models process the\ndata independently, offering diverse analytical perspectives.\nHigher-level models then collectively analyze their outputs to\nenhance predictive accuracy. Each subsequent level integrates\nand improves upon the previous ones [33], reducing biases\nand variances, thus enhancing the overall predictive accuracy.\nFig. 2 visually represents the multi-layer stacking approach,\npresenting the flow from Level-0 to Level-N layers.\n4) REGRESSION ANALYTICS\nThe Autoregressive Integrated Moving Average (ARIMA)\nmodel is essential for time-series analysis, effectively\ncapturing temporality with autoregressive features and mov-\ning averages for non-seasonal forecasting [34]. The Sea-\nsonal Autoregressive Integrated Moving Average (SARIMA)\nbuilds on ARIMA by incorporating seasonality, making it\nadept at predicting seasonal fluctuations in time series data.\n78234\nVOLUME 12, 2024"
  },
  {
    "page_number": 4,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 2. Multi-layer stacking in ensemble modeling. This figure depicts\nthe layered architecture of our ensemble model, from initial Level-0\nmodels to advanced aggregation layers, highlighting the sequential\nprediction and refinement process.\nExponential Smoothing (ETS) is a time-honored technique\nfor forecasting that assigns exponentially decreasing weights\nover time. It captures trends and seasonalities in time-series\ndata [35], and its simplicity makes it particularly effective for\nquick and short-term forecasting.\nLinear Regression is a fundamental model in statistical\nanalysis [36] and is ideal for predicting a continuous\ndependent variable using one or more independent variables.\nIt assumes a linear relationship between the variables.\nA simple LR involves a single independent variable, whereas\nmultiple LRs extend to multiple variables. This model is\nparticularly effective in scenarios where a linear correlation\nexists, with coefficients indicating the influence of each\nvariable.\nRandom Forest creates multiple decision trees to improve\nthe prediction accuracy and robustness. It leverages bagging\nand feature randomness in tree construction [37], reducing\noverfitting and enhancing diversity. This method effectively\nhandles complex datasets, offering a reliable performance in\nvarious applications.\nFeedforward Neural Networks feature a straightforward\ninput, hidden, and output layer structure [38]. Using sequen-\ntial neuron connections, they excel in complex pattern\nrecognition and prediction. Highly versatile and efficient,\nthey are ideal for handling large datasets. Their widespread\nuse in applications such as speech recognition in predictive\nanalytics stems from their proficient nonlinear modeling of\nintricate relationships between inputs and outputs.\nConvolutional Neural Networks (CNN), renowned for their\nefficacy in image processing, also excel in time-series and\nregression tasks owing to their ability to prioritize pat-\nterns through convolutional layers [39]. It enables practical\napplications in audio processing and natural language tasks,\nextending their use beyond visual data analysis.\nTemporal Convolutional Networks (TCN) combine the\nstrengths of CNN with the temporal sensitivity suited for\nsequence modeling [40]. They effectively handle long input\nsequences in complex time series forecasting tasks.\nRecurrent Neural Networks (RNN) are fundamental mod-\nels for sequential data analysis. It has the unique ability\nto retain information across sequences [41], making it\nessential for text processing, speech recognition, and time-\nseries analysis. Nevertheless, RNN encounters challenges\nwith long-term dependencies, leading to the development of\nadvanced versions, such as Gated Recurrent Unit (GRU) and\nLong Short-Term Memory (LSTM). The GRU incorporates\na simplified gating mechanism to address the vanishing gra-\ndient problem [42], balancing computational efficiency with\nthe ability to learn long-term dependencies. This advantage\nmakes the GRU suitable for complex tasks such as language\nmodeling and nuanced time-series analysis. Alternatively,\nLSTM selectively retains or forgets information [43], making\nit practical for extended sequences. Its capability to handle\nlong-term dependencies makes LSTM ideal for challenging\ntasks, such as predictive text generation, advanced time series\nforecasting, and language translation.\n5) CLASSIFICATION ANALYTICS\nDecision Trees are known to simplify complex decision-\nmaking. They created an intuitive, tree-like structure by seg-\nmenting datasets into branches based on feature values [44].\nThis method divides data into smaller, more manageable\nsubsets, effectively handling nonlinear relationships among\nvariables and facilitating hierarchical decision processes.\nA Support Vector Machine (SVM) is used in classification\nand regression tasks. It offers robust performance in high-\ndimensional spaces owing to its margin maximization prin-\nciple, which forms a hyperplane for data classification [45].\nK-Nearest\nNeighbors\n(KNN)\nis\nan\ninstance-based\nalgorithm that classifies data by analyzing the majority class\namong its ‘‘K’’ nearest neighbors, relying on a similarity\nprinciple [46]. The algorithm’s accuracy is sensitive to the\nchosen number of neighbors and the specific distance metric\napplied, thereby impacting its adaptability to various data.\nLogistic Regression, essential for binary classification in\nmachine learning [47], uses a logistic function to convert\npredictor variables into probabilities, thereby accommodat-\ning nonlinear variable relationships. In various fields, like\nmedical diagnosis and spam detection, employing maximum\nlikelihood estimation for coefficient calculation is pivotal.\nNeural Networks, inspired by the structure of biological\nneurons, are composed of complex interconnected layers\nof artificial neurons or nodes [48]. It begins with an input\nlayer, includes one or more hidden layers, and ends with\nan output layer. Each node simulates a biological neuron,\ncalculates a weighted sum of inputs, and applies a nonlinear\ntransformation commonly termed the activation function.\n6) EVALUATION METHOD\nThe Root Mean Square Error (RMSE) is a critical metric\nfor assessing regression tasks, quantifying the square root of\nthe mean of the squared discrepancies between the predicted\nand actual values. By squaring the errors before averaging,\nthe RMSE heavily penalizes the larger discrepancies, making\nit particularly sensitive to significant prediction errors.\nThis characteristic enhances its utility in offering a precise\nmeasure of model accuracy. The formula is detailed in (1)\nRMSE =\nv\nu\nu\nt1\nn\nn\nX\nk=1\n(yk −ˆyk)2\n(1)\nVOLUME 12, 2024\n78235"
  },
  {
    "page_number": 5,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 3. cSysGuard architecture. This figure demonstrates the process of system preprocessing, data collection, performance metrics\nforecasting, health assessment, and continuous model refinement.\nwhere yk represents the actual value, ˆyk denotes the predicted\nvalue, and n signifies the number of observations. A lower\nRMSE indicates better accuracy compared to a higher one.\nThe F1 score is a key metric for evaluating classification\ntasks. It is computed by harmonically averaging precision,\nwhich is the ratio of true positives to all positive predictions,\nindicating the accuracy of positive classifications, and recall,\nthe ratio of true positives to all actual positives, reflecting the\nability to identify all relevant instances. These calculations\nare elaborated in (2), (3), and (4) for class n.\nPrecisionn =\nTPn\nTPn + FPn\n(2)\nRecalln =\nTPn\nTPn + FNn\n(3)\nF1-Scoren = 2 × Precisionn × Recalln\nPrecisionn + Recalln\n(4)\nIn addition to these equations, TP represents true positives,\nFP signifies false positives, and FN indicates false negatives.\nAn F1 score of 1 signifies perfect precision and recall,\nindicating that all predictions are accurate and complete.\nConversely, a score of 0 represents the lowest accuracy, where\nthe model fails to identify any true positive values.\nIII. METHODOLOGY OVERVIEW\nThis section provides a detailed overview of the methodology\nused in our study, focusing on the design and data handling\nof cSysGuard.\nA. CSYSGUARD ARCHITECTURE\nFig. 3 comprehensively depicts the architecture of cSys-\nGuard,\ncomprising\nsix\ncore\ncomponents:\napplication\nFIGURE 4. cSysGuard execution workflow. The figure illustrates\ncSysGuard’s operational sequence, which mainly encompasses the stages\nof preprocessing, data prediction, and model refinement.\nsimulation, data aggregator, controller, metrics and health\npredictor, ensemble refiner, and preprocessor. Each is integral\nto the prediction process. The following sections briefly\ndescribe the system workflow and each element, elucidating\ntheir functions and collaborative interactions.\n1) EXECUTION WORKFLOW\nFig. 4 reveals the execution workflow of cSysGuard.\nInitially, cSysGuard determines whether the configuration\nnecessitates the creation of models. In such cases, it initiates\nand stores these models for future analysis; otherwise,\nit bypasses the preprocessing step and proceeds directly to\nthe primary process using the pre-existing models stored in\nthe repository. In the main phase, the system remains idle\nuntil a task emerges. Upon reaching the prediction intervals,\nthe algorithm collects data from the target application\nto access the most recent inputs and then activates the\n78236\nVOLUME 12, 2024"
  },
  {
    "page_number": 6,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\npredictors to generate and store the forecast. In addition\nto preserving model performance, the refinement process is\nactivated at predefined intervals to utilize the latest dataset\nto align models with current data trends. Post-refinement,\nupdated models are stored. This cycle of prediction and\nrefinement recurs over time, ensuring cSysGuard’s processes\nare perpetually optimized and up-to-date.\n2) APPLICATION SIMULATION\nThis component emulates an application cloud environment,\nproviding a means to acquire the dataset. The study leveraged\nDigital Ocean’s cloud service [49], deploying three Linux\nvirtual machines [50], each with 1vCPU, 1GB of memory,\nand a 10GB disk. The API gateway is responsible for\nrouting requests to the stateless application service, which\nutilizes a database for data storage. Our target service\nincorporates a monitoring tool called Prometheus [51] to\ncollect and store metrics over time methodically. In addition,\nthe simulator employs a custom script to simulate dynamic\nworkloads, manipulating the intensity of various metrics in\nunique combinations for each request. For instance, one\nscenario may involve high CPU and memory usage with low\nusage in other metrics, whereas another could stress high\nbandwidth usage while keeping the rest low. This approach\nenables the model to evaluate the system’s performance under\nfluctuating conditions, focusing on randomness and swift\nchanges. These assumptions mirror real-world cloud envi-\nronments, enhancing the generalizability of cSysGuard by\nensuring it can adapt to unpredictable and rapidly changing\nworkloads.\n3) DATA AGGREGATOR\nThis component is responsible for updating and structuring\nthe dataset for modeling and analysis. Initially, the process\naggregates the system data from the application service\nand then transforms the metrics into a suitable format for\npredictive modeling. To enhance the accuracy of the fore-\ncasts, it also performs a noise reduction process to minimize\ndata inconsistencies. Finally, the component forwards the\nprocessed dataset to the controller for further processing.\n4) CONTROLLER\nThis component is crucial for orchestrating cSysGuard’s\nlogic flow. It begins by fetching the essential setting\nparameters, such as the model refinement and prediction\nschedules, from the configuration repository. The controller\ninitiates the models to generate system metrics and health\nforecasts. Afterward, it archives the predictions in the\nmetrics repository. Beyond these tasks, it manages the model\nmaintenance by periodically signaling the refiner to tune\nmodels as per the schedule. Additionally, it consistently\nacquires the latest data from the aggregator for system\npredictions and refinements. This adept coordination by the\ncontroller is vital to cSysGuard’s seamless operation and\noverall dependability and efficiency.\n5) METRICS AND HEALTH PREDICTOR\nThis component mainly oversees the prediction process,\nincluding forecasting system performance metrics and health\nassessments. Once activated by the controller, the prediction\nprocess is initiated with the most recent dataset as the input.\nRepresented as an ensemble regression model, the first three\nlayers—Level 0, Level 1, and Weight Aggregation—mainly\nfocus on forecasting performance metrics, as detailed in\nSection IV. Besides, we incorporate several sets of predictors\nwithin the forecasting layers, with each set specializing\nin specific metrics. Upon completion, it consolidates and\nforwards the results to the final layer for system health\nassessment, requiring inputs from all metric sets for a\nthorough evaluation, as detailed in Section V. Moreover,\neach layer sends its predictions to the controller for storage,\nserving as valuable data for subsequent model evaluation and\nrefinement. This component ensures precise and timely pre-\ndictions through a comprehensive and systematic ensemble\narchitecture.\n6) ENSEMBLE REFINER\nThis component maintains the regression model’s proficiency\nin identifying the trends and data characteristics. Upon\nactivation by the controller, the refiner fine-tunes the Level-0\nand Level-1 models by utilizing the latest dataset. Similarly,\nit adjusts the Level-1 weights based on the range of\nhistorical predictions, dynamically aligning the models’\nstrengths with their predictive efficacy. Once completed,\nthe system promptly updates the repository with the newly\nrefined model and weights. Through this iterative refinement,\nthe component enhances the adaptability and precision of\nthe models, ensuring they remain effective in a dynamic\nenvironment.\n7) PREPROCESSOR\nThis component is essential for setting up the models\ncSysGuard requires to initiate the primary process. The\npreprocessor commences by retrieving the list of models\nfrom the system configuration. Then, it actively compiles\nthe models with predefined parameters and training datasets.\nUpon completion, the calibrated models are stored in the\nrepository. However, the preprocessing step is optional if the\nrepository already contains the models.\nB. DATA PREPROCESSING\nIn the data collection, we implemented a custom script\nto access Prometheus’ local storage to receive resource\nutilization and performance indicators. The dataset [57] com-\nprises approximately 8,000 data points, each accumulated\nevery 5 seconds. The metrics include CPU and memory\nrequests (expressed as percentages), network inbound and\noutbound rates (measured in gigabytes per second, GB/s,\nor megabytes per second, MB/s), transactions per second\n(TPS), and response time (in seconds, s, or milliseconds, ms),\nas illustrated in Fig. 5. Moreover, we collected the system’s\nVOLUME 12, 2024\n78237"
  },
  {
    "page_number": 7,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nTABLE 1. Transformed system performance metrics dataset with server health in 40 seconds.\nFIGURE 5. System performance metrics across different parameters. This\nfigure displays a series of graphs that showcase CPU and memory\nrequests, inbound and outbound bandwidth, transaction rates, and\naverage response times, all measured over a 10-minute period.\nreal-time health status, which was determined by scrutinizing\nHTTP response codes [53]. When the script detects prede-\nfined error response codes, it classifies the system health\nas ‘‘unhealthy’’; in all other cases, it designates the status\nas ‘‘healthy.’’ Although stateless applications rarely leverage\ndisk storage, which is not part of the training input, we still\nmonitor it to ensure its proper stateless behavior.\nSubsequently, the system performs a series of transfor-\nmations to format the data appropriately for subsequent\nmodeling and analysis. Table 1 shows the modification\nresults, outlining how various metrics were adjusted to meet\nthe analytical requirements. Specifically, we normalized CPU\nand memory requests to a scale from 0 to 1, transformed\ninbound and outbound bandwidth in megabytes per second\n(MB/s), and quantified response time in milliseconds (ms).\nWe retained the TPS feature in its original form as collected.\nThe mechanism also transformed system status from the orig-\ninal textual designations into a numerical format, encoding\n‘‘healthy’’ as 0 and ‘‘unhealthy’’ as 1, facilitating binary\nanalysis.\nFIGURE 6. Comparative predictions and RMSE with/without low-pass\nbutterworth filter. This figure displays the 10-minute predictions for LSTM,\nGRU, and ARIMA models in CPU requests, illustrating the comparison\nbetween scenarios with filtered and unfiltered data.\nEventually, the system performs a noise reduction process\nutilizing a low-pass Butterworth filter to minimize data\ninconsistencies. To demonstrate an enhanced forecasting\naccuracy, Fig. 6 showcases the impact of noise reduction by\ncontrasting the predictions and RMSE of LSTM, GRU, and\nARIMA models using both noise-filtered and raw data inputs\nwith actual values. It highlights the enhancement achieved by\napplying a filter to reduce noise. The process then yields and\nstores a well-refined dataset for further analysis.\nIV. SYSTEM METRICS FORECASTING\nThis section describes the detailed process of forecast-\ning performance metrics. This comprehensive examination\n78238\nVOLUME 12, 2024"
  },
  {
    "page_number": 8,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 7. Regression prediction sequence. This figure displays the\nprediction cycle, integrating o observations and p forecast steps within a\nset interval k over n predictive cycles.\ncovers the intricacies of Level-0 and Level-1 learners, Weight\nAggregation, and the approaches used for comparing models\nand tuning hyperparameters in regression models.\nA. LEVEL-0 LEARNER\nLevel-0 learners comprise base models designed for time-\nseries prediction that utilize datasets aggregated directly\nfrom the system for their training. At each predefined\ninterval, these models use the most recent dataset to predict\nforthcoming values, considering two key elements: the\npredetermined count of past data points (observation steps)\nand the quantity of future data points (prediction steps) to\nbe predicted. To illustrate, Fig. 7 visualizes the sequence\nof time series predictions, showcasing observation steps\n‘‘o’’ and prediction steps ‘‘p’’ for each interval ‘‘k’’ across\n‘‘n’’ predictions. cSysGuard allows the adjustment of these\nparameters to meet user-specific requirements. In this study,\nwe configured the settings with 30 observation steps and five\nforecast steps, aligning the interval with the forecasting steps.\nOur setup has operated various models, including the\nARIMA, SARIMA, ETS, CNN, TCN, RNN, GRU, and\nLSTM. Each brings a unique forecasting approach, enabling\nus to capture various aspects and patterns within the data.\nIn addition to deep learning, we incorporated two hidden\nlayers into the architecture: the first equipped with 64 neurons\nand the second with 32 neurons, both employing ReLU\nactivation. We used the default settings provided by the\nStatsmodels [54] and TensorFlow [55] libraries. Despite\nthis, we observed that the library’s default parameter values\nfor deep learning led to suboptimal predictions. Therefore,\nparameter adjustments are required. Table 2 outlines the\nmodel-fitting configuration, ensuring the retention of initial\nlearning proficiency of deep learning models. Upon predic-\ntion, the models forward the results to the subsequent layer,\nLevel 1.\nB. LEVEL-1 LEARNER\nLevel-1 learners serve as meta-models designed for refining\nthe regression outputs and constitute the second layer in our\npredictive framework. Their main objective is to improve the\nforecast precision by leveraging the outcomes from Level-0\nmodels as inputs to produce refined outputs. This integration\nenables Level-1 learners to fine-tune the initial forecasts,\nenhancing the overall predictive performance of cSysGuard.\nTABLE 2. Parameters influencing performance in deep learning model\ntraining.\nThis layer comprises three distinct models: Linear Regres-\nsion, Random Forest, and Feedforward Neural Networks.\nLinear Regression provides simplicity and interoperability\nto effectively capture linear relationships within the data.\nRandom Forest contributes to reducing overfitting and\nimproving generalization. Feedforward Neural Networks\nexcel at capturing complex, nonlinear patterns and interac-\ntions in the data. This combination leverages the strengths\nof each model, ensuring that the stacking approach can\neffectively generalize across different metrics and system\nbehaviors to enhance the predictive accuracy and robustness\nof cSysGuard.\nFor Linear Regression and Random Forest, we employed\nthe standard parameters provided by the Scikit-learn\nlibrary [56]. For Feedforward Neural Networks, we replicated\nthe deep learning structure and model-fitting parameters of\nLevel-0 models using the TensorFlow library.\nTo optimally balance cSysGuard’s predictive performance\nand computational complexity, we adopted a selective\napproach to determine which Level-0 models would be fed\ninto specific Level-1 models for predictions. The selection\ndepends on the impact each model has on the predictive\nperformance for particular metrics, evaluated by the Pearson\ncorrelation coefficient, r [58], which is defined as follows:\nr =\nP(xi −¯x)(yi −¯y)\npP(xi −¯x)2 P(yi −¯y)2\n(5)\nwhere xi represents the predicted metric values from Level-0\nmodels, ¯x signifies the average predicted metric across Level-\n0 models, yi indicates the actual metric values, and ¯y reflects\nthe mean of the actual metrics. Upon score calculation,\nwe sorted the models in descending order based on their\ncorrelations to gauge their impact. For instance, Fig. 9\nillustrates the Pearson correlations of each base model for\npredicting CPU requests. We then incrementally incorporated\nLevel-0 models into each Level-1 learner, starting with\nthose having the highest correlation scores, and assessed\nVOLUME 12, 2024\n78239"
  },
  {
    "page_number": 9,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 8. Incremental performance improvement in level-1 models. This figure illustrates RMSE progression for Level-1 models predicting CPU requests\n(unit: percent), highlighting the incremental performance enhancements achieved with each addition of a Level-0 model.\nFIGURE 9. Correlation analysis of level-0 models in CPU request\nprediction. This figure ranks the base models based on the Pearson\ncorrelation scores.\nFIGURE 10. Consolidated performance analysis of level-1 models. This\nfigure synthesizes the RMSE achievements of Level-1 models in\nforecasting CPU requests (unit: percent), encapsulating the collective\nefficacy derived from integrating multiple Level-0 models.\nthe predictive performance of each Level-1 model using\nthe RMSE. This iterative process continued until all base\nmodels were utilized and assessed. For example, Fig. 8\ndemonstrates the improvement in the performance of Level-\n1 models for predicting CPU requests by adding each\nmodel. Alternatively, Fig. 10 offers a unified visualization\nof the enhancements across the models, presented in a\nsingle comprehensive graph. Our analysis aimed to pinpoint\nan ‘‘elbow point’’ [59], where incorporating additional\nmodels led to minimal gains in the predictive accuracy\nfor the specific metric. This selective approach to model\nintegration significantly conserves computational resources\nwhile ensuring high predictive precision, demonstrating a\nsuccessful trade-off between performance and computational\nefficiency in intricate forecasting tasks.\nC. WEIGHTED AGGREGATION LAYER\nThe weighted aggregation layer is the final regression stage\nfor predicting forthcoming metrics. To effectively leverage\nthe contributions of each Level-1 model, the system receives\npredictive model outputs and refines the final prediction\nusing a weight-averaging method [60] by assigning higher\nweights to more accurate models. In addition to the weight\ncalculation, the layer computes the Performance Vector (PV)\nof Level-1 learners based on past t refinement iterations. The\nPV is represented as a 3 × t matrix:\nPV =\n\n\nPELR,1\nPELR,2\n· · ·\nPELR,t\nPERF,1\nPERF,2\n· · ·\nPERF,t\nPEFF,1\nPEFF,2\n· · ·\nPEFF,t\n\n\n(6)\nwhere it consolidates the prediction errors, measured by\nRMSE, for Linear Regression (LR), Random Forest (RF),\nand Feedforward Neural Networks (FF) across t refinement\niterations. The refinement process enables ongoing updates\nto the model performance, ensuring sustained accuracy in a\nrapidly changing cloud environment.\nNevertheless, incorporating the RMSE directly as weights\npresents a significant challenge, as it may not adequately\nreflect model correlations. A model with a slightly lower\nRMSE may not enhance the ensemble diversity if similar\nto another model. To mitigate this issue, we developed\na normalized weighting system derived from the PV.\nThe calculation scales the errors into normalized weights,\nidentified as NormWeighti,j, converting them to a range\nbetween 0 and 1 for comparability across different models.\nThe weight normalization formula is outlined in (7):\nNormWeighti,j =\nexp(−α × PEi,j)\nP\nk∈{LR,RF,FF}\nexp(−α × PEk,j)\n(7)\nwhere the weights are calculated by inversely relating them\nto their RMSE, expressed as exp(−α × PEk,j), ensuring that\nmodels with lower errors are more reliable than those with\nhigher errors. In addition, α acts as a sensitivity controller,\nadjusting the weight assignment based on error magnitude.\nα facilitates a subtle balance between emphasizing and mod-\nerating differences in model errors and acts as a controllable\nsafeguard to prevent integer overflow [61] during exponential\nsystem computations. To maximize the weight difference, our\n78240\nVOLUME 12, 2024"
  },
  {
    "page_number": 10,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nstudy aimed to maximize each weight value while ensuring\nthat outcomes stayed within an acceptable range as controlled\nby α. This strategy allows for a more pronounced distinction\nin model weights, which is particularly beneficial when the\nmodels’ performances are nearly identical.\nTo derive the final prediction, we integrated the outputs\nfrom our Level-1 learners through a weighted averaging\napproach. The final prediction for a given time point m,\nrepresented by Pm, is determined as (8):\nPm =\nX\nk∈{LR,RF,FF}\nNormWeightk,j × Pk,m\n(8)\nwhere each model’s prediction, denoted as Pk,m, is multiplied\nby its respective normalized weight, NormWeightk,m. This\nprocess guarantees that each model’s contribution to the final\nprediction is proportional to its performance. By harmonizing\nthe models’ strengths, this approach yields a more precise and\nreliable ensemble forecast.\nD. MODEL COMPARISON\nOur approach evaluated cSysGuard’s efficacy in forecasting\nsystem metrics by comparing its performance with traditional\nmodels used in the Level-0 layer. We utilized RMSE to\ncompare and benchmark the performance across different\nmetrics. This method clearly explains the precision and\ndependability of cSysGuard, emphasizing its capability to\nmanage dynamic environments effectively.\nE. HYPERPARAMETER TUNING\nAfter comparing the models, we concentrated on devel-\noping a strategy to enhance cSysGuard further. Therefore,\nwe selected one model from each Level-0 and Level-1 layer\nand investigated their impacts by adjusting the hyperpa-\nrameters through Bayesian optimization [62]. Subsequently,\nwe evaluated the improvements using the RMSE as our\nbenchmark.\nGiven the system configuration constraints outlined in\nSection VI-E, we limited our hyperparameter tuning to\nparticular models and metrics to mainly focus on a methodol-\nogy optimizing cSysGuard. With an emphasis on enhancing\nCPU request predictions, the study focused on refining the\nRNN and Random Forest models, designated as Level-0\nand Level-1, respectively. These models were selected not\nonly due to their numerous tunable parameters but also to\ndemonstrate that our hyperparameter tuning methodology\nis effective across deep learning and traditional machine\nlearning techniques in a stacking architecture.\nThe hyperparameters covered various factors within speci-\nfied ranges, as detailed in Table 3, directly affecting model\nperformance. For the RNN, these parameters include data\nstructuring elements (such as observation steps), model\narchitecture (covering learning rate, the number of neurons,\nand activation functions in both the first and second layers),\nand model training configurations (incorporating epochs and\nbatch size). For the Random Forest, considered parameters\nentail the number of trees (estimators), tree depth (max\nTABLE 3. Range of hyperparameters for recurrent neural networks (RNN)\nand random forest (RF) tuning in CPU request analysis.\ndepth), minimum samples for a split (min sample split),\nminimum samples per leaf (min sample leaf), maximum\nnumber of features considered for splitting a node (max\nfeatures), maximum number of leaf nodes (max-leaf nodes),\nminimum impurity decrease for a split (min impurity\ndecrease), whether bootstrap samples are used (bootstrap),\nand the function to measure the quality of a split (criterion).\nV. SYSTEM HEALTH ASSESSMENT\nThis\nsection\ndescribes\nthe\nsystem\nhealth\nassessment\napproach, focusing on layer concepts, model comparisons,\nand hyperparameter tuning in the classification models.\nA. HEALTH PREDICTION LAYER\nThe health predictor represents the last layer in our pre-\ndictive framework. It has a classification model designed\nto determine the system’s health. Based on inputs from the\nforecasting layer that precedes it, this model categorizes the\nsystem’s condition as either ‘‘healthy’’ or ‘‘unhealthy.’’\nMoreover, we utilized the sliding window technique to\nincorporate data trends into the prediction process to enhance\npredictive accuracy. To forecast the target point, the model\nconcentrates on the latest data within a specified range,\ndefined by the window size. As illustrated in Fig. 11,\ncSysGuard adopts this technique for system health assess-\nment, concentrating on the ten latest forecast points from\nthe regression layer to ascertain the current system status.\nThis method effectively captures the temporal patterns of\nVOLUME 12, 2024\n78241"
  },
  {
    "page_number": 11,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nFIGURE 11. Sliding window technique in classification prediction. This\nfigure demonstrates the sliding window technique for system health\nassessment, featuring the window size ws of regression predictions rp\nover n cycles, resulting in classification predictions cp.\ndata, leading to more accurate forecasts of future system\nconditions.\nIn addition to the imbalanced nature of our dataset [63],\ncharacterized by a higher prevalence of ‘‘healthy’’ statuses,\nthere is a risk of models favoring the majority class and\nunderperforming on the crucial but rarer ‘‘unhealthy’’ class.\nTo address this, we implemented SMOTE to enhance the\nrepresentation of the minority class, thereby enhancing the\nmodel equity by creating synthetic instances of this class to\nimprove its detection.\nB. MODEL COMPARISON\nAn essential aspect of our methodology is the comparative\nanalysis of various machine learning models to assess\ntheir performance efficacy. For this analysis, we selected\nfive prominent models: Decision Trees, Support Vector\nMachines, K-Nearest Neighbors, Logistic Regression, and\nNeural Networks. These models were initially performed\nwith the default parameters provided by the Scikit-learn\nand TensorFlow libraries [55], [56]. In addition to Neural\nNetworks, our configuration featured a two-layered structure:\nthe first layer had six nodes using the ReLU activation\nfunction, and the second comprised a single node with a\nSigmoid activation function.\nTo enhance the model’s generalizability, the evaluation\nprocess employed 10-fold cross-validation, conducting sev-\neral training rounds and validation on different data splits\nto derive the average scores. We utilized precision, recall,\nand F1 score with macro averaging [64], ensuring equal\nweighting of each category in the assessment throughout the\ncross-validation process. The model with the highest macro-\naveraged F1 score is the most optimal, as it demonstrates\na balanced trade-off between precision and recall, with the\nspecifics detailed in (9), (10), and (11).\nPrecisionmacro = Precisionhealthy + Precisionunhealthy\n2\n(9)\nRecallmacro = Recallhealthy + Recallunhealthy\n2\n(10)\nTABLE 4. Hyperparameter ranges in decision tree tuning.\nF1-Scoremacro = 2 × Precisionmacro × Recallmacro\nPrecisionmacro + Recallmacro\n(11)\nC. HYPERPARAMETER TUNING\nSubsequently, we carefully refined the model’s hyperparam-\neters through Bayesian optimization to enhance its predictive\ncapabilities with our dataset. We methodically tested a range\nof parameter values to identify the combination that resulted\nin the highest macro-averaged F1 score. This process allowed\nus to optimize the model’s accuracy and tailor the model\nspecifically to the nuances of the dataset.\nTable 4 presents the hyperparameters of the Deci-\nsion Trees, identified as the top-performing model in\nSection VI-B. These influential hyperparameters encompass\nmax depth, min sample split, min sample leaf, criterion, max\nfeatures, max-leaf nodes, and min impurity decrease.\nVI. RESULT AND DISCUSSION\nThis section presents our empirical findings for machine-\nlearning models in system metrics and health forecasting.\nIt also provides an insightful discussion of the findings.\nA. SYSTEM METRICS FORECASTING EVALUATION\nThis subsection details the experimental results of the system\nmetrics forecasting, including two main categories: model\ncomparison and hyperparameter tuning.\n1) MODEL COMPARISON\nTable 5 provides a detailed performance evaluation of\ncSysGuard compared to the selected Level-0 models across\nvarious metrics. Deep learning models, such as GRU, LSTM,\nand RNN, show competitive predictive performance due\nto their ability to capture complex temporal dependencies\nand nonlinear patterns in the data. In contrast, traditional\nmodels like ARIMA and SARIMA generally lag in most\nmetrics because they rely on more straightforward, linear\nassumptions and are less effective at handling the vari-\nability and complexity of dynamic cloud environments.\nOverall, cSysGuard shows superior proficiency in most areas.\nNotably, in response time, it achieves a significant 2.28-fold\nincrease in performance compared to ETS. Notably, this\ncomparison also highlights cSysGuard’s capability to surpass\nadvanced deep learning models, such as GRU and LSTM,\ntypically known for their strong performance in predictive\n78242\nVOLUME 12, 2024"
  },
  {
    "page_number": 12,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nTABLE 5. RMSE-based performance comparison of cSysGuard with selected level-0 models across specific metrics.\nFIGURE 12. System performance metrics prediction. This figure compares\ncSysGuard’s prediction results with those of commonly used models\n(GRU, RNN, LSTM, ARIMA, CNN) against actual results for CPU and\nmemory requests, bandwidth, transaction rates, and response times.\ntasks. These results demonstrate that cSysGuard effectively\nsurpasses traditional models in certain areas, reinforcing\nits robustness and adaptability in diverse system-metric\nforecasting scenarios.\nAdditionally, Fig. 12 visualizes the predicted values\ncompared to the actual values for various metrics. Each graph\ncompares the performance of cSysGuard with commonly\nused models such as GRU, RNN, LSTM, ARIMA, and\nCNN. The visualizations clarify cSysGuard’s predictive capa-\nbilities, highlighting its effectiveness in forecasting diverse\nsystem metrics. Given their alignment with performance\nbenchmarks and the vast array of adjustable parameters,\nwe opted for RNN and Random Forest as our Level-0 and\nLevel-1 models in our optimization experiment.\n2) HYPERPARAMETER TUNING\nTable 6 lists the optimal RNN and Random Forest parameters\nfor CPU usage prediction. Simultaneously, Table 7 presents\nthe improved performance of each model and cSysGuard.\nInterestingly, the tuning process not only augmented the indi-\nvidual performances of the RNN and Random Forest but also\nTABLE 6. Optimal parameters for recurrent neural networks (RNN) and\nrandom forest (RF) tuning in CPU request analysis.\nTABLE 7. RMSE-based performance of recurrent neural networks (RNN),\nrandom forest (RF), and cSysGuard in CPU request (unit: percent).\nenhanced the predictive capabilities of the subsequent layers\nin the architecture. This finding suggests a synergistic effect,\nwhere refining one system component positively influences\noverall efficacy, thereby underscoring the interconnected\nnature of the models within the system.\nB. SYSTEM HEALTH ASSESSMENT EVALUATION\nThis subsection specifically delves into the experimental\nresults of the system health assessment, dividing them into\nmodel comparison and hyperparameter tuning.\nVOLUME 12, 2024\n78243"
  },
  {
    "page_number": 13,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nTABLE 8. Performance of base models and tuned decision trees in\nsystem health assessment.\nTABLE 9. Optimal parameters for tuning decision trees.\n1) MODEL COMPARISON\nTable 8 presents a comparative analysis of various base\nmodels’ performance, focusing on the macro-averaged\nprecision, recall, and F1 score. The analysis reveals that\nthe Decision Trees outperform the other base models by\nachieving the highest F1 score. This performance highlights\nits efficiency in accurately identifying positive instances\nwhile minimizing false positives, as evidenced by its high\nrecall and precision. Such a balanced effectiveness profile\nunderscores the suitability of Decision Trees for reliable\nsystem health status prediction, establishing it as a robust\nchoice for such assessments. Based on the evaluation,\nwe focused on Decision Trees for in-depth analysis during\nhyperparameter tuning.\n2) HYPERPARAMETER TUNING\nWith Decision Trees identified as the top-performing base\nmodel, Table 8 offers a comparative analysis showcasing\nthe impact of hyperparameter tuning on model performance.\nIn addition, Table 9 presents the optimal parameter settings of\nour model. The results indicate that the optimized Decision\nTrees outperform the original model, showing enhancements\nof approximately 1.56% in precision, 1.66% in recall,\nand 1.67% in the F1 score. It demonstrates the improved\nperformance of its original version and other base models.\nFollowing\nthe\nhyperparameter\ntuning\nprocess,\nwe\nembarked on a detailed analysis of the model’s classification\naccuracy. We constructed an averaged confusion matrix,\ndenoted as Fig. 13. It synthesized the outcomes from each\niteration of cross-validation conducted with our sample\nFIGURE 13. Confusion matrix visualization for system health assessment.\nThis figure illustrates the averaged confusion matrix, capturing the\npredictive accuracy of server status as healthy or unhealthy.\ndata, offering a transparent and quantifiable measure of\ncSysGuard’s predictive accuracy. The matrix delineates two\nprincipal classes indicative of server status: ‘‘healthy’’ and\n‘‘unhealthy.’’ In this matrix, columns represent predicted\nlabels, and rows correspond to actual labels. This graphical\nrepresentation is crucial for evaluating the precision and\nreliability of our model in distinguishing the server’s\noperational states.\nC. COMPARATIVE ANALYSIS OF DIFFERENT ENSEMBLE\nMETHODOLOGY\nOur study introduces an innovative ensemble stacking model\nto enhance predictive accuracy. In detail, the regression\nframework is a three-tiered architecture, comprising base\nmodels in the first layer, meta-models in the second, and\nemploying a weighted averaging approach in the third. Then,\nwe conducted a comparative analysis incorporating existing\nresearch studies alongside a novel experiment we devised, all\nutilizing similar ensemble modeling approaches.\nSpecifically, we have selected two research studies.\nBoth employ two-tiered architectural frameworks. The first\nliterature, by Mehmood et al. [23], proposed an ensemble\nmodel with a single meta-model (Decision Trees) in the\nsecond layer, which leverages predictions from the base\nmodels to determine the outcome. The second publication,\nby Kim et al. [22], introduced an ensemble model framework\nnamed CloudInsight. It incorporates a weighted average\nstrategy in its second layer to aggregate and summarize\npredictions from the base models. In addition, we conducted\nan experiment that involves simple averaging in the third\nlayer of cSysGuard. The study aimed to determine if a\nmore streamlined model structure or a more straightforward\naggregation method could achieve superior results without\nadding extra complexity. Therefore, we constructed these\nframeworks and applied them to our dataset to determine\n78244\nVOLUME 12, 2024"
  },
  {
    "page_number": 14,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nTABLE 10. RMSE performance comparison: Single Meta-Model,\nCloudInsight, and cSysGuard (simple averaging: cSG-SA & weighted\naveraging: cSG-WA) across metrics.\ntheir effectiveness and compare them to the cSysGuard\narchitecture.\nTable 10 presents the comparative effectiveness of each\nmethodology. Employing CloudInsight or a single meta-\nmodel does not match the performance efficacy of cSys-\nGuard’s configuration (cSG-WA). Alternatively, while using\na simple averaging strategy in cSysGuard (cSG-SA) aligns\nclosely with the current version, the critical advantage of\nweighted averaging is its flexibility; it enables users to\ntailor weight calculations based on specific needs, such as\nLevel-1 model error sensitivity or collected error range.\nThis flexibility enhances overall performance to favor better-\nperforming models by fine-tuning their weights, emphasizing\nthe importance of cSysGuard’s nuanced three-tier layering\nstrategy.\nD. cSysGuard PERFORMANCE OVERHEAD\nSubsequently, our analysis shifted towards assessing the\nimpact on system performance, specifically examining the\ncomputational overhead. We experimented with cSysGuard\non a host machine with a 2.9 GHz Intel Core i7-7820HK\nQuad-Core processor and 16GB DDR4 RAM. Table 11 show-\ncases a comprehensive comparison, highlighting differences\nin key performance indicators such as average prediction\nand refinement times across three machine learning models\n(RNN, GRU, and LSTM) chosen for their substantial\noperational times and the cSysGuard (cSG) framework.\nBased on the result, cSysGuard exhibits marginally higher\nprocessing times than the selected high-performing models.\nHowever, it is essential to note that cSysGuard’s processing\nduration is contingent upon the complexity of the underlying\nmodel it incorporates. Employing a more streamlined model\nwithin the system can reduce overall processing times for\ncSysGuard. Conversely, based on our findings, enhancing\nthe host’s resources can also reduce overall processing time.\ncSysGuard is fundamentally influenced by the characteristics\nTABLE 11. Comparative analysis of prediction and refinement times\n(unit: seconds) for Selected ML models and cSysGuard (cSG).\nof the employed models and the capabilities of its operating\nmachine.\nE. SYSTEM SPECIFICATION BIAS\nAs Section III-A2 outlined, our methodology deployed\nthe system with specific configurations, including CPU\ntype, memory capacity, and the operating system used.\nIt is essential to acknowledge that differences in these\nspecifications can influence the overall modeling process\nand its predictive outcomes. Such variations, in turn, can\npotentially affect the predictive accuracy and generalizability\nof cSysGuard.\nTherefore, the study recommends that users consider\nadjusting cSysGuard’s setup to suit their server specifi-\ncations. Users can select different models and quantities,\noptimize parameters, fine-tune data configuration and pre-\ndiction interval, or adjust Level-1 model error sensitivity.\ncSysGuard offers flexibility to align with users’ operational\ngoals and requirements, maximizing the advantages of\nensemble learning.\nVII. CONCLUSION AND FUTURE WORK\nThis paper introduces cSysGuard, a novel and resilient\nmonitoring framework designed to enhance the prediction\nof system performance metrics and health assessments\nin dynamic cloud environments. To forecast performance\nmetrics effectively, cSysGuard implements an advanced\nensemble model using a stacking approach that integrates\nvarious predictors, such as the Level-0 and Level-1 models.\nSubsequently, the framework dynamically yields an accu-\nrate final metric prediction through a weighted averaging\nstrategy. Furthermore, cSysGuard examines these forecasts to\ndetermine system failover through a fine-tuned classification\nmodel.\nIn our study, cSysGuard consistently outperformed the\nother standard models, encompassing traditional statistical\nand advanced deep learning approaches. It showcased sig-\nnificant proficiency, from nearly equivalent to an impressive\nincrease of up to 2.28 times compared to conventional\nmodels. In addition to the system health assessment, it lever-\naged Decision Trees refined through hyperparameter tuning,\nachieving a macro-averaged F1 score of 89.79%. These\noutcomes highlight cSysGuard’s effectiveness in delivering\nprecise performance metrics and health analysis, showcasing\nit as a comprehensive system monitoring tool.\nIn future work, we intend to expand the capabilities\nof cSysGuard beyond its focus on detecting system-health\nfailures based on resource sufficiency. Future iterations will\nVOLUME 12, 2024\n78245"
  },
  {
    "page_number": 15,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\nconsider integrating other system failures, such as network-\nrelated issues, security breaches, and application-level errors.\nBy addressing these additional dimensions, cSysGuard will\nbecome a more versatile framework adaptable to a broader\nrange of scenarios and capable of providing comprehensive\nmonitoring across various aspects of system health.\nREFERENCES\n[1] VMware\nby\nBroadcom.\nWhat\nis\nCloud\nScalability?\nAccessed:\nJan. 14, 2024. [Online]. Available: https://www.vmware.com/topics/\nglossary/content/cloud-scalability.html\n[2] Synopsys. How Cloud Computing Flexibility Enables Design Changes.\nAccessed: Jan. 14, 2024. [Online]. Available: https://www.synopsys.\ncom/cloud/insights/cloud-computing-flexibility.html\n[3] Red Hat. (Dec. 21, 2023). Stateful vs Stateless. Accessed: Jan. 14, 2024.\n[Online].\nAvailable:\nhttps://www.redhat.com/en/topics/cloud-native-\napps/stateful-vs-stateless\n[4] Q. Zhang and R. Boutaba, ‘‘Dynamic workload management in hetero-\ngeneous cloud computing environments,’’ in Proc. IEEE Netw. Oper.\nManage. Symp. (NOMS), Krakow, Poland, May 2014, pp. 1–7, doi:\n10.1109/NOMS.2014.6838288.\n[5] L. Rosencrance, S. Louissaint, and K. Brush. (2024). Service-Level\nAgreement (SLA). TechTarget. Accessed: Jan. 25, 2024. [Online].\nAvailable: https://www.techtarget.com/searchitchannel/definition/service-\nlevel-agreement\n[6] M. A. S. Netto, C. Cardonha, R. L. F. Cunha, and M. D. Assuncao,\n‘‘Evaluating auto-scaling strategies for cloud computing environments,’’\nin Proc. IEEE 22nd Int. Symp. Modeling, Anal. Simulation Com-\nput. Telecommun. Syst., Paris, France, Sep. 2014, pp. 187–196, doi:\n10.1109/MASCOTS.2014.32.\n[7] Amazon Web Services. (Jul. 4, 2023). EC2 Auto Scaling. [Online].\nAvailable:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-\nscale-based-on-demand.html\n[8] Google Cloud. Autoscaling Groups of Instances. Accessed: Jun. 14, 2023.\n[Online]. Available: https://cloud.google.com/compute/docs/autoscaler\n[9] P. Wenda. (Jul. 2, 2021). AutoScaling: Introducing Compute Engine\nPredictive Autoscaling. Google Cloud. Accessed: Jan. 14, 2024. [Online].\nAvailable: https://cloud.google.com/blog/products/compute/introducing-\ncompute-engine-predictive-autoscaling\n[10] Google Cloud. Scaling Based on Predictions. Accessed: Jan. 14, 2024.\n[Online].\nAvailable:\nhttps://cloud.google.com/compute/docs/\nautoscaler/predictive-autoscaling\n[11] N. Chairatana and R. Chawuthai, ‘‘Cloud stateless server failover predic-\ntion using machine learning on proactive system metrics,’’ in Proc. 18th\nInt. Joint Symp. Artif. Intell. Natural Lang. Process. (iSAI-NLP), Bangkok,\nThailand, Nov. 2023, pp. 1–6, doi: 10.1109/isai-nlp60301.2023.10354585.\n[12] S. Niu, J. Zhai, X. Ma, X. Tang, and W. Chen, ‘‘Cost-effective cloud HPC\nresource provisioning by building semi-elastic virtual clusters,’’ in Proc.\nInt. Conf. High Perform. Comput., Netw., Storage Anal., Denver, CO, USA,\nNov. 2013, pp. 1–12.\n[13] A. A. Bankole and S. A. Ajila, ‘‘Predicting cloud resource provisioning\nusing machine learning techniques,’’ in Proc. 26th IEEE Can. Conf. Electr.\nComput. Eng. (CCECE), Regina, SK, Canada, May 2013, pp. 1–4, doi:\n10.1109/CCECE.2013.6567848.\n[14] N. Suksriupatham and A. Hoonlor, ‘‘Workload prediction with regression\nfor over and under provisioning problems in multi-agent dynamic resource\nprovisioning framework,’’ in Proc. 17th Int. Joint Conf. Comput. Sci.\nSoftw. Eng. (JCSSE), Bangkok, Thailand, Nov. 2020, pp. 128–133, doi:\n10.1109/JCSSE49651.2020.9268289.\n[15] R. N. Calheiros, E. Masoumi, R. Ranjan, and R. Buyya, ‘‘Workload\nprediction using ARIMA model and its impact on cloud applications’\nQoS,’’ IEEE Trans. Cloud Comput., vol. 3, no. 4, pp. 449–458, Oct. 2015,\ndoi: 10.1109/TCC.2014.2350475.\n[16] K. Dmytro, T. Sergii, and P. Andiy, ‘‘ARIMA forecast models for\nscheduling usage of resources in IT-infrastructure,’’ in Proc. 12th Int.\nSci. Tech. Conf. Comput. Sci. Inf. Technol. (CSIT), vol. 1, Lviv, Ukraine,\nSep. 2017, pp. 356–360, doi: 10.1109/STC-CSIT.2017.8098804.\n[17] P. Sekwatlakwatla, M. Mphahlele, and T. Zuva, ‘‘Traffic flow prediction\nin cloud computing,’’ in Proc. Int. Conf. Adv. Comput. Commun.\nEng. (ICACCE), Durban, South Africa, Nov. 2016, pp. 123–128, doi:\n10.1109/ICACCE.2016.8073735.\n[18] D. Lee, D. Lee, M. Choi, and J. Lee, ‘‘Prediction of network\nthroughput using ARIMA,’’ in Proc. Int. Conf. Artif. Intell. Inf.\nCommun.\n(ICAIIC),\nFukuoka,\nJapan,\nFeb.\n2020,\npp. 1–5,\ndoi:\n10.1109/ICAIIC48513.2020.9065083.\n[19] S. T. Singh, M. Tiwari, and A. S. Dhar, ‘‘Machine learning based workload\nprediction for auto-scaling cloud applications,’’ in Proc. OPJU Int.\nTechnol. Conf. Emerg. Technol. Sustain. Develop. (OTCON), Chhattisgarh,\nIndia, Feb. 2023, pp. 1–6, doi: 10.1109/OTCON56053.2023.10114033.\n[20] J. Gao, H. Wang, and H. Shen, ‘‘Machine learning based workload\nprediction in cloud computing,’’ in Proc. 29th Int. Conf. Comput.\nCommun. Netw. (ICCCN), Honolulu, HI, USA, Aug. 2020, pp. 1–9, doi:\n10.1109/ICCCN49398.2020.9209730.\n[21] E. Caron, F. Desprez, and A. Muresan, ‘‘Forecasting for grid and cloud\ncomputing on-demand resources based on pattern matching,’’ in Proc.\nIEEE 2nd Int. Conf. Cloud Comput. Technol. Sci., Nov. 2010, pp. 456–463,\ndoi: 10.1109/CLOUDCOM.2010.65.\n[22] I. K. Kim, W. Wang, Y. Qi, and M. Humphrey, ‘‘Forecasting cloud appli-\ncation workloads with CloudInsight for predictive resource management,’’\nIEEE Trans. Cloud Comput., vol. 10, no. 3, pp. 1848–1863, Jul. 2022, doi:\n10.1109/TCC.2020.2998017.\n[23] T. Mehmood, S. Latif, and S. Malik, ‘‘Prediction of cloud computing\nresource utilization,’’ in Proc. 15th Int. Conf. Smart Cities, Improving\nQuality Life Using ICT IoT (HONET-ICT), Islamabad, Pakistan, Oct. 2018,\npp. 38–42, doi: 10.1109/HONET.2018.8551339.\n[24] S. Li, J. Bi, H. Yuan, M. Zhou, and J. Zhang, ‘‘Improved LSTM-based\nprediction method for highly variable workload and resources in clouds,’’\nin Proc. IEEE Int. Conf. Syst., Man, Cybern. (SMC), Toronto, ON, Canada,\nOct. 2020, pp. 1206–1211, doi: 10.1109/SMC42975.2020.9283029.\n[25] S. Bhagavathiperumal and M. Goyal, ‘‘Workload analysis of cloud\nresources\nusing\ntime\nseries\nand\nmachine\nlearning\nprediction,’’\nin\nProc.\nIEEE\nAsia–Pacific\nConf.\nComput.\nSci.\nData\nEng.\n(CSDE),\nMelbourne,\nVIC,\nAustralia,\nDec.\n2019,\npp. 1–8,\ndoi:\n10.1109/CSDE48274.2019.9162385.\n[26] P. N, S. Chhetri, S. R. Kini, G. G. Mishra, and S. Kumar, ‘‘Resource\nprovisioning in cloud using ARIMA and LSTM technique,’’ in Proc. IEEE\n2nd Mysore Sub Sect. Int. Conf. (MysuruCon), Mysuru, India, Oct. 2022,\npp. 1–7, doi: 10.1109/MysuruCon55714.2022.9972689.\n[27] W. Chen, C. Lu, K. Ye, Y. Wang, and C.-Z. Xu, ‘‘RPTCN: Resource\nprediction for high-dynamic workloads in clouds based on deep learning,’’\nin Proc. IEEE Int. Conf. Cluster Comput. (CLUSTER), Portland, OR, USA,\nSep. 2021, pp. 59–69, doi: 10.1109/Cluster48925.2021.00038.\n[28] M. Hassan, H. Chen, and Y. Liu, ‘‘DEARS: A deep learning\nbased\nelastic\nand\nautomatic\nresource\nscheduling\nframework\nfor\ncloud\napplications,’’\nin\nProc.\nIEEE\nInt.\nConf.\nParallel\nDistrib.\nProcess.\nAppl.,\nUbiquitous\nComput.\nCommun.,\nBig\nData\nCloud\nComput.,\nSocial\nComput.\nNetw.,\nSustain.\nComput.\nCommun.\n(ISPA/IUCC/BDCloud/SocialCom/SustainCom),\nMelbourne,\nVIC,\nAustralia, Dec. 2018, pp. 541–548, doi: 10.1109/BDCLOUD.2018.00086.\n[29] A. Radwan, M. Amarneh, H. Alawneh, H. I. Ashqar, A. AlSobeh, and\nA. A. A. R. Magableh, ‘‘Predictive analytics in mental health leveraging\nLLM embeddings and machine learning models for social media analysis,’’\nInt. J. Web Services Res., vol. 21, no. 1, pp. 1–22, Feb. 2024, doi:\n10.4018/ijwsr.338222.\n[30] J. H. Challis, ‘‘Signal processing,’’ in Experimental Methods in Biome-\nchanics. Cham, Switzerland: Springer, 2021, doi: 10.1007/978-3-030-\n52256-8_4.\n[31] L. D. Paarmann, ‘‘Butterworth filters,’’ in Design and Analysis of Analog\nFilters (The International Series in Engineering and Computer Science),\nvol. 617. Boston, MA, USA: Springer, 2003, doi: 10.1007/0-306-48012-\n3_3.\n[32] S. Satpathy. (Nov. 17, 2023). SMOTE for Imbalanced Classification\nWith Python. Analytics Vidhya. Accessed: Jan. 28, 2024. [Online].\nAvailable:\nhttps://www.analyticsvidhya.com/blog/2020/10/overcoming-\nclass-imbalance-using-smote-techniques\n[33] R. Dey and R. Mathur, ‘‘Ensemble learning method using stacking with\nbase learner: A comparison,’’ in Proc. Int. Conf. Data Anal. Insights,\nvol. 727. Singapore: Springer, 2023, pp. 159–169, doi: 10.1007/978-981-\n99-3878-0_14.\n[34] A. C. Harvey, ‘‘ARIMA models,’’ in The New Palgrave (Time Series and\nStatistics), J. Eatwell, M. Milgate, and P. Newman, Eds. London, U.K.:\nPalgrave Macmillan, 1990, doi: 10.1007/978-1-349-20865-4_2.\n[35] R. J. Hyndman, A. B. Koehler, J. K. Ord, and R. D. Snyder, Forecasting\nWith Exponential Smoothing: The State Space Approach. Cham, Switzer-\nland: Springer, 2008, doi: 10.1007/978-3-540-71918-2.\n78246\nVOLUME 12, 2024"
  },
  {
    "page_number": 16,
    "text": "N. Chairatana, R. Chawuthai: Stateless System Performance Prediction and Health Assessment\n[36] G. James, D. Witten, T. Hastie, and R. Tibshirani, ‘‘Linear regression,’’\nin An Introduction to Statistical Learning (Springer Texts in Statistics).\nNew York, NY, USA: Springer, 2021, doi: 10.1007/978-1-0716-1418-1_3.\n[37] Y. Liu, Y. Wang, and J. Zhang, ‘‘New machine learning algorithm: Random\nforest,’’ in Information Computing and Applications (Lecture Notes in\nComputer Science), vol. 7473, B. Liu, M. Ma, and J. Chang, Eds. Berlin,\nGermany: Springer, 2012, doi: 10.1007/978-3-642-34062-8.\n[38] S. Skansi, ‘‘Feedforward neural networks,’’ in Introduction to Deep Learn-\ning (Undergraduate Topics in Computer Science). Cham, Switzerland:\nSpringer, 2018, doi: 10.1007/978-3-319-73004-2_4.\n[39] S. Tripathy and R. Singh, ‘‘Convolutional neural network: An overview\nand application in image classification,’’ in Proc. 3rd Int. Conf. Sustain.\nComput., vol. 1404. Singapore: Springer, 2022, doi: 10.1007/978-981-16-\n4538-9_15.\n[40] C. Lea, R. Vidal, A. Reiter, and G. D. Hager, ‘‘Temporal convolutional\nnetworks: A unified approach to action segmentation,’’ in Proc. Comput.\nVis. ECCV Workshops, in Lecture Notes in Computer Science, vol. 9915.\nCham, Switzerland: Springer, 2016, pp. 47–54.\n[41] S. A. Marhon, C. J. F. Cameron, and S. C. Kremer, ‘‘Recurrent neural\nnetworks,’’ in Handbook on Neural Information Processing (Intelligent\nSystems Reference Library), vol. 49, M. Bianchini, M. Maggini, and\nL. Jain, Eds. Berlin, Heidelberg: Springer, 2013, doi: 10.1007/978-3-642-\n36657-4_2.\n[42] S.\nKostadinov.\n(Dec.\n16,\n2017).\nUnderstanding\nGRU\nNetworks.\nTowards Data Sci. Accessed: Jan. 17, 2024. [Online]. Available:\nhttps://towardsdatascience.com/understanding-gru-networks-\n2ef37df6c9be\n[43] K. Smagulova and A. P. James, ‘‘Overview of long short-term memory\nneural networks,’’ in Deep Learning Classifiers With Memristive Networks\n(Modeling and Optimization in Science and Technologies), vol. 14,\nA. James, Ed. Cham, Switzerland: Springer, 2020, doi: 10.1007/978-3-\n030-14524-8_11.\n[44] J. R. Quinlan, ‘‘Induction of decision trees,’’ Mach. Learn., vol. 1, no. 1,\npp. 81–106, Mar. 1986, doi: 10.1007/bf00116251.\n[45] C. Cortes and V. Vapnik, ‘‘Support-vector networks,’’ Mach. Learn.,\nvol. 20, pp. 273–297, Jul. 1995, doi: 10.1007/BF00994018.\n[46] T. Cover and P. Hart, ‘‘Nearest neighbor pattern classification,’’ IEEE\nTrans. Inf. Theory, vol. IT-13, no. 1, pp. 21–27, Jan. 1967, doi:\n10.1109/TIT.1967.1053964.\n[47] J. S. Cramer, ‘‘The origins and development of the logit model,’’ in Logit\nModels From Economics and Other Fields. Cambridge, U.K.: Cambridge\nUniv. Press, 2003, pp. 149–157, doi: 10.1017/CBO9780511615412.010.\n[48] B. Mer, J. Reinhardt, and M. T. Strickland, ‘‘Neural networks:\nAn introduction,’’ in Physics of Neural Networks, 2nd ed. Berlin, Germany:\nSpringer, 1995, doi: 10.1007/978-3-642-57760-4.\n[49] Digital\nOcean.\nAccessed:\nJan.\n14,\n2024.\n[Online].\nAvailable:\nhttps://www.digitalocean.com\n[50] Google Cloud. What is a Virtual Machine? Accessed: Jan. 14, 2024.\n[Online].\nAvailable:\nhttps://cloud.google.com/learn/what-is-a-virtual-\nmachine\n[51] Prometheus. Accessed: Jun. 15, 2023. [Online]. Available: https://\nprometheus.io\n[52] Zabbix.\nAccessed:\nMay\n26,\n2023.\n[Online].\nAvailable:\nhttps://\nwww.zabbix.com\n[53] MDN\nWeb\nDocs:\nHTTP\nResponse\nStatus\nCodes.\nAccessed:\nJun. 15, 2023. [Online]. Available: https://developer.mozilla.org/en-U.S./\ndocs/Web/HTTP/Status\n[54] Statsmodels. Accessed: Jan. 15, 2023. [Online]. Available: https://www.\nstatsmodels.org\n[55] TensorFlow. Accessed: Jan. 15, 2023. [Online]. Available: https://www.\ntensorflow.org\n[56] Scikit-learn. Accessed: Jan. 15, 2023. [Online]. Available: https://www.\nscikit-learn.org\n[57] N. Chairatana and R. Chawuthai, Jan. 31, 2024, ‘‘Cloud stateless system\nperformance metrics and status,’’ IEEE Dataport, doi: 10.21227/8wf2-\n2y40.\n[58] S. Turney. (Jun. 22, 2023). Pearson Correlation Coefficient. Scribbr.\nAccessed: Jan. 15, 2024. [Online]. Available: https://www.scribbr.com/\nstatistics/pearson-correlation-coefficient\n[59] B. Saji. (2021). Elbow Method for Finding the Optimal Number of\nClusters in K-Means. Analytics Vidhya. Accessed: Jan. 15, 2024.\n[Online]. Available: https://www.analyticsvidhya.com/blog/2021/01/in-\ndepth-intuition-of-k-means-clustering-algorithm-in-machine-learning\n[60] D. Schumacher. Understanding Weighted Average. SERP AI. Accessed:\nJan. 15, 2024. [Online]. Available: https://serp.ai/weighted-average\n[61] ScienceDirect.\nInteger\nOverflow.\nAccessed:\nJan.\n15,\n2024.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/topics/computer-\nscience/integer-overflow\n[62] W.\nWang.\n(2020).\nBayesian\nOptimization\nConcept\nExplained\nin\nLayman Terms. Towards Data Sci. Accessed: Jan. 15, 2024. [Online].\nAvailable:\nhttps://towardsdatascience.com/bayesian-optimization-\nconcept-explained-in-layman-terms-1d2bcdeaf12f\n[63] J. Brownlee. (2020). A Gentle Introduction to Imbalanced Classification.\nMach. Learn. Mastery. Accessed: Jun. 15, 2023. [Online]. Available:\nhttps://machinelearningmastery.com/what-is-imbalanced-classification\n[64] A. Tariq. (2023). What is the Difference Between Micro and Macro\nAveraging? Educative. Accessed: Jun. 15, 2023. [Online]. Available:\nhttps://www.educative.io/answers/what-is-the-difference-between-micro-\nand-macro-averaging\nNUTT CHAIRATANA received the B.Eng. degree\nin computer innovation engineering from the King\nMongkut’s Institute of Technology Ladkrabang,\nBangkok, Thailand, in 2022, where he is cur-\nrently pursuing the M.Eng. degree in robotics\nand computational intelligence systems with the\nDepartment of Robotics and AI Engineering.\nFrom 2022 to 2023, he was a Software Engineer\nwith RentSpree. He is a Machine Learning Engi-\nneer with Kasikorn Business-Technology Group,\nThailand.\nRATHACHAI CHAWUTHAI received the B.Eng.\ndegree in computer engineering from the King\nMongkut’s Institute of Technology Ladkrabang,\nBangkok, Thailand, in 2006; the M.Eng. degree in\ninformation management from Asian Institute of\nTechnology, Pathum Thani, Thailand, in 2012; and\nthe Ph.D. degree in informatics from SOKENDAI\nand National Institute of Informatics, Kanagawa,\nJapan, in 2016.\nFrom 2006 to 2010, he was a Software Engineer\nwith Thomson Reuters. From 2012 to 2013, he was a Senior Software\nEngineer with Punsarn Asia. From 2013 to 2016, he was a Research Assistant\nwith the National Institute of Informatics. He is currently an Assistant\nProfessor with the King Mongkut’s Institute of Technology Ladkrabang.\nVOLUME 12, 2024\n78247"
  }
]